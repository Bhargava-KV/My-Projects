{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07558a7e",
   "metadata": {},
   "source": [
    "## Bag of words model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3593248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the libraries\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc117c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a paragraph for data\n",
    "\n",
    "paragraph = '''\n",
    "What makes me angry is that when someone is not on time. I have always had a habit to be on time and when someone doesn't do the same I am really angry.\n",
    "\n",
    "It is with my friends as well I give them time to meet me at 2 for instance and they are late I will leave and will not wait for them any more minute.\n",
    "\n",
    "I think everyone should make it their habit to be on time because it is a good and healthy activity. Making someone wait is not a good habit.\n",
    "\n",
    "The reason why this makes me angry is that this indicates you are not respecting the other person, the other person should be your priority and in order to show someone that they are really important and they matter to you, respect them by meeting them on the time you gave them.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9557fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " ## importing librarties for cleaning data \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1297ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning data \n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "wordnet = WordNetLemmatizer()\n",
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = re.sub('[^a-zA-z]', ' ', sentences[i])\n",
    "    words = words.lower()\n",
    "    words = words.split()\n",
    "    words = [wordnet.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    words = ' '.join(words)\n",
    "    corpus.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07aebc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the bagof words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9eefd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
